# -*- coding: utf-8 -*-
"""CS473-Assignment_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16oDRmDztxdnUnBJKCmk1Dlmq7u7QzIxt

# **CS-473 (Pattern Recognition)**

## Assignment 4

##### Nikolaos Kougioulis (csdp1285)

---


### **Question 1** 

The scope of this exercise is to get you familiar with training a linear classifier with Gradient Descent.

**Data:** For this exercise, you will use the dataset in the dataset.csv file that consists of 1000 2-dimensional data samples and their labels.

**Equation:**
$$\text{learning_rate}_i = \frac{1}{\sqrt{k_i}}$$

where $k_i = k_{i-1} + (i \mod n), ~k_0 = 0, i \geq 1, i = \text{iteration index}, n = \text{number of samples.}$
"""

!python --version

import math 
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt

from tqdm import tqdm
import random

from google.colab import files
uploaded = files.upload()

df = pd.read_csv("dataset.csv", sep=",")
df = df.iloc[: , 1:] #remove id collumn
print(df.head())

"""**Questions:**

1. Create a scatter plot of the dataset. Is the dataset linearly separable? Justify your answer in 1-2 sentences.

**Solution**
"""

labels = df["labels"]

plt.scatter(df["feature_0"], df["feature_1"], c=labels)
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Scatter Plot')
plt.show()

"""The provided data are not linearly separable, as there is no distinct hyperplane (in this case since we are dealing with two dimensional features, a line) that clearly separates the two classes. Notice that in the middle of the plot, some points coincide with the cluster of the points in the opposite class, thus making it clear that a linear classifier, eg a perceptron, would not perform adequately. 

In more detail, given a set of observations $\left\{ (\mathbf{x}_i, y_i \right\}_{i=1,2,\ldots,n}$ where $\mathbf{x}$ is a vector of features and $y_i \in \left\{-1,1\right\}$ the corresponding binary class of the feature vector $\mathbf{x}_i$, the set is said to be *linearly separable* if there exists a vector of weights $\mathbf{w}$ and a bias term $b$ such that the corresponding hyperplane perfectly separates the two classes, that is for an observed feature vector $\mathbf{x}$,

$$y = \text{sign}(\mathbf{w} \cdot \mathbf{x} + b) = \begin{cases} \text{class 1}, ~\text{if} ~\mathbf{w} \cdot \mathbf{x} + b >0 \\ \text{class -1}, ~\text{if} ~\mathbf{w} \cdot \mathbf{x} + b < 0 \end{cases}$$

2. Create a function called *train single sample* that implements the Fixed-Increment Single-Sample Perceptron algorithm. The arguments of the function should be:

 (a) a := weights + bias (bias trick)

 (b) y := data + 1s (bias trick)

 (c) labels

 (d) n iterations := the number of iterations/updates

 (e) lr := learning rate

 (f) variable lr := boolean 
 
 and the function should return:

 (a) a := trained model

 (b) acc history := list of accuracy values

The algorithm should be implemented according to the notes. Extra functionalities include computing and printing the model's accuracy every *n_samples* iterations. Also, you should save and return only the best
model. **Bonus 2.5%:** Use the tqdm library to print the progress of the model along with the accuracy during training.

**Solution**

The pseudocode for the fixed-increment single-sample perceptron is the following [1]:

**Algorithm** fixed-increment single-sample perceptron:

---
**begin:** initialize $\mathbf{a}, ~k=0$
**do** $k = (k+1)\text{mod}n$ where $n$, is the number of samples

  **if** $\mathbf{y}_k$ is misclassified by $\mathbf{a}$ then update $\mathbf{a}$ by $\mathbf{a} \leftarrow a - \lambda \mathbf{y}_k$

  **until** all patterns are properly classified or after a fixed number of iterations

**end**

where $\lambda$ is the learning rate.

We utilize the *tqdm* library for monitoring the training progress using a tqdm iterator object and compute the accuracy every *n_samples* iterations and print using *tqdm.write*. The function *train_single_sample is illustrated bellow:
"""

def train_single_sample(a, y, labels, n_iterations, lr, variable_lr):
    acc_history = []
    best_accuracy = float("-inf")
    best_model = None
    
    labels = np.where(labels == 0, -1, labels) #change labels to -1,1 for practical reasons
    n_samples = len(labels)

    for iteration in tqdm(range(n_iterations), desc="Single-sample Perceptron Training"):
        correct_predictions = 0
        #print(iteration)
        k = iteration % n_samples
        x = y[k].astype(a.dtype)
        label = labels[k]
            
        if np.sign(np.dot(a, x)) != label:
            a = a - lr * x.astype(a.dtype)
      
        #compute accuracy every n_samples iterations
        if (iteration) % n_samples == 0: 
            accuracy = 0
            for j in range(n_samples):
                x = y[j].astype(a.dtype)
                label = labels[j]
                if np.sign(np.dot(a, x)) == label:
                    accuracy += 1
            accuracy /= n_samples
            acc_history.append(accuracy)
            tqdm.write(f"Accuracy: {accuracy:.2f}")

            if accuracy > best_accuracy:
                best_accuracy = accuracy
                best_model = a 

    if best_model is None:
        best_accuracy = 0
        best_model = np.zeros_like(a)
    
    return best_model, acc_history

"""3. Create a function called *plot model* that takes as input the trained weights (+ bias), the data, and the labels and returns a scatter plot with the decision boundaries of the model (Hint: Use contourf)

**Solution**

After generating a meshgrid, we compute the predicted class for each point in the meshgrid and color the decision boundary using contourf. This is illustrated in the *plot_model* function below:
"""

def plot_model(a, y, labels):
    x_1 = np.linspace(-10, 10, 100)
    x_2 = np.linspace(-10, 10, 100)
    xx_1, xx_2 = np.meshgrid(x_1, x_2)
    Z = np.zeros((len(x_1), len(x_2)))
    for i in range(len(x_1)):
        for j in range(len(x_2)):
            Z[i, j] = np.dot(a, np.array([x_1[i], x_2[j], 1]))
    Z = np.sign(Z)

    plt.contourf(xx_1, xx_2, Z, cmap=plt.cm.Paired, alpha=0.8)

    plt.scatter(y[:, 0], y[:, 1], c=labels, cmap=plt.cm.Paired)

    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.legend()
    plt.show()

"""4. Train a linear model using the functions you have implemented. Use the following hyperparameters: (a) n iterations = 100000 (b) lr = 100000 (c) variable lr = False

**Solution**

We add 1s using the bias trick and call the function with the requested hyperparameters:
"""

y = df[['feature_0', 'feature_1']].to_numpy()
labels = df['labels'].to_numpy()

#print(y.shape)
#print(labels.shape)

#add 1s (bias trick)
#y = np.hstack((y, np.ones((y.shape[0], 1))))
bias_trick = np.ones((len(y), 1))
y = np.concatenate((y, bias_trick), axis=1)

#initial weights and bias (including bias trick)

a = np.random.rand(1,3) #random 1x3 np array
a[:-1] = 1 #bias trick

n_iterations = 10**5
lr = 1
variable_lr = False 

best_model, acc_history = train_single_sample(a, y, labels, n_iterations, lr, variable_lr)

"""5. Plot the history of the accuracy during training.

**Solution**
"""

print("Best Model:", best_model)
print("Accuracy History:", acc_history)
#print("Best accuracy:", max(acc_history))
plt.plot(acc_history)
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Single-sample perceptron")
plt.show()

"""6. Plot the data along and the trained model. Use the *plot model* function you implemented before.

**Solution**

This question is answered by calling the *plot_model* function defined previously.
"""

plot_model(best_model,y,labels)

"""7. Now we are going to retrain our model but with a variable learning rate. Create a *Scheduler* class that implements a function *get next lr* that every time it is called returns the next learning rate. The object should work according to the function at the beginning of the assignment. Test your object by initializing it and plotting the learning rate over 100 steps. Now configure the training function to use this object when *variable lr = True*

**Solution**

Our implemented scheduler class is the following, where $\text{learning_rate}_i = \frac{1}{\sqrt{k_i}}$ with $k_i = k_{i-1} + (i \mod n), ~k_0 = 0, i \geq 1, i = \text{iteration index}, n = \text{number of samples.}$
"""

class Scheduler:
    def __init__(self, lr, n_samples):
        self.lr = lr
        self.n_samples = n_samples
        self.k = 0

    def get_next_lr(self):
        lr = 1 / np.sqrt(self.k + 1)
        self.k = self.k + (i % self.n_samples)
        return lr

"""We can confirm our scheduler works as expected by initializing the learning rate to $1$ and then plotting the decaying learning rate for 100 steps."""

scheduler = Scheduler(lr=1, n_samples=100)

rates = []
for i in range(100):
    learning_rate = scheduler.get_next_lr()
    rates.append(learning_rate)

plt.plot(range(1, 101), rates)
plt.xlabel('Iteration')
plt.ylabel('Learning Rate')
plt.title('Variable Learning Rate for 100 iterations')
plt.show()

"""8. Retrain the model with a variable learning rate. Plot the dataset and the trained model as before. What is the main difference compared to training with a fixed learning rate? What method do you think is better? Justify your answer.

**Solution**

We re-write the *train_single_sample* function in order to take into account the Scheduler class and when the *variable_lr* argument is equal to True.

The main difference betwen training with a fixed learning rate and a variable (also called decaying) learning rate is that instead of updating the weights by a constant factor at each iteration, which often leads to slower convergence of the optimization method and being trapped in a local extrema instead of convering to the global extrema. In many cases, learning with a variable learning rate allows for faster convergence of the approximation method (in our case, the update rule of the perceptron learning is gradient descent optimization) and allows to avoid being trapped in local extrema like the fixed learning rate. All in all, although learning is an optimization problem and not an explicitely defined one, as it heavily depends on the starting configurations at $t=0$ and the characteristics of the data, variable learning rate is often preffered due to its added flexibility.



---


[Versloot, C. (2019,November 11) Problems with fixed and decaying learning rates](https://github.com/christianversloot/machine-learning-articles/blob/main/problems-with-fixed-and-decaying-learning-rates.md)

[Lau, S. (2017, August 1). Learning Rate Schedules and Adaptive Learning Rate Methods for Deep Learning](https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1)
"""

def train_single_sample(a, y, labels, n_iterations, lr, variable_lr):
    acc_history = []
    best_accuracy = float("-inf")
    best_model = None
    
    labels = np.where(labels == 0, -1, labels)
    n_samples = len(labels)

    if variable_lr:
        scheduler = Scheduler(lr, n_samples)

    for iteration in tqdm(range(n_iterations), desc="Single Sample Perceptron Training"):
        correct_predictions = 0
        
        k = (iteration +1) % n_samples
        x = y[k].astype(a.dtype)
        label = labels[k]
            
        if np.sign(np.dot(a, x)) == -1:
          a += lr * x.astype(a.dtype)

          if variable_lr:
            lr = scheduler.get_next_lr()
      
        #compute the accuracy every n_samples iterations
        if iteration % n_samples == 0:
            accuracy = 0
            for j in range(n_samples):
                x = y[j].astype(a.dtype)
                label = labels[j]
                if np.sign(np.dot(a, x)) == label:
                    accuracy += 1
            accuracy /= n_samples
            acc_history.append(accuracy)
            tqdm.write(f"Accuracy: {accuracy:.2f}")

            if accuracy > best_accuracy:
                best_accuracy = accuracy
                best_model = a 

        if best_model is None:
         best_accuracy = 0
         best_model = np.zeros_like(a)
    
    return best_model, acc_history

a = np.random.rand(1,3)
a[:-1] = 1 #bias trick

n_iterations = 10**5 
lr = 1
variable_lr = True

best_model, acc_history = train_single_sample(a, y, labels, n_iterations, lr, variable_lr)

print("Best Model:", best_model)

plt.plot(acc_history)
plt.xlabel("Iterations per sample count - Epochs")
plt.ylabel("Accuracy")
plt.title("Single-sample perceptron")

plt.show()

plot_model(best_model,y,labels)

"""### **Question 2** 

The data, the equation, and the notice from the previous question apply to this one too.

**Questions:**

1. Create a function called *train batch* that implements the Batch Perceptron algorithm. The arguments of the function should be:

 (a) the same as the *train single sample* in Question 1 +
 (b) theta := the value for the theta criterion
 (c) batch size and the function should return:
 (a) the same as the *train single sample* in Question 1 +
 (b) error history := list of error values

 The algorithm should be implemented according to the notes. Extra functionalities include computing and n samples printing the model's accuracy and the error every batch size iterations. Also, you should save and return only the best model. The error here is the absolute sum of the updating step, that is the value we use to update the weights. **Bonus 2.5%:** Use the tqdm library to print the progress of the model along with the accuracy and the error during training.

**Solution**

This question is similar to the previous ones, except we check for misclassified samples and update the gradient using batches of features instead of a single feature. Batching is performed using sampling without replacement, and the update step is performed by summing over the misclassified samples. In more detail [1]:

**Algorithm** fixed-variable-increment batch perceptron:

---
**begin:** initialize $\mathbf{a}, ~\eta(k), ~k=0, ~\text{threshold} ~\theta$

**do** $k = (k+1)$

 $$\displaystyle \mathbf{a} \leftarrow a + \eta(k) \sum_{y \in \mathcal{Y}_k}  \mathbf{y}$$

  $$\textbf{until} ~~\displaystyle \eta(k) \sum_{y \in \mathcal{Y}_k}  \mathbf{y} < \theta$$

**end**

where $\eta(k)$ is the fixed-variable learning rate dependent on iteration k.

This is implemented in the *train_batch* function, with the two additional parameters *batch_size* (size of the randomly sampled batch, without replacement) and *theta* (theta termination criterion):
"""

def train_batch(a, y, labels, n_iterations, lr, variable_lr, batch_size, theta):
    acc_history = []
    error_history = []
    best_accuracy = float("-inf")
    best_model = None
    
    labels = np.where(labels == 0, -1, labels)
    n_samples = len(labels)
    
    for iteration in tqdm(range(n_iterations), desc="Batch perceptron training"):
        error = np.zeros_like(a)
        
        indices = np.random.choice(n_samples, size=batch_size, replace=False)
        batch_y = y[indices]
        batch_labels = labels[indices]
        
        for k in range(batch_size):
            x = batch_y[k].astype(a.dtype)
            label = batch_labels[k]
            
            if np.sign(np.dot(a, x)) != label:
                error += x.astype(a.dtype)
        
        a += lr * error
        
        #compute accuracy and abs error every n_samples / batch_size iterations
        if (iteration % (n_samples / batch_size)) == 0:
          accuracy = np.mean([np.sign(np.dot(a, x)) == label for x, label in zip(y, labels)])
          acc_history.append(accuracy)
          tqdm.write(f"Accuracy: {accuracy:.2f}")
          error_history.append(np.abs(np.sum(error)))
          tqdm.write(f"Absolute Error: {np.sum(np.abs(error)):.2f}")

          if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_model = np.copy(a)
        
        if np.sum(np.abs(error)) <= theta:
            tqdm.write("Terminating due to the theta criterion")
            break

        if best_model is None:
         best_accuracy = 0
         best_model = np.zeros_like(a)
    
    return best_model, acc_history, error_history

"""2. Train a linear model using the function *train batch* you have implemented. Use the following hyperparameters:

 (a) n_iterations = 100000

 (b) theta = 0.01

 (c) batch_size = 16

 (d) lr = 100000

 (e) variable_lr = False

**Solution**
"""

a = np.random.rand(1,3) #including bias trick
a[:-1] = 1

n_iterations = 10**5
lr = 1
theta = 0.01
batch_size = 16
variable_lr = False

best_model, acc_history, error_history = train_batch(a, y, labels, n_iterations, lr, variable_lr, batch_size, theta)

print(best_model)

"""3. Plot the history of the accuracy and the error during training. Use the *plt.subplot* function.

**Solution**

As asked, using the *plt.subplot* function:
"""

plt.subplot(2, 1, 1)
plt.plot(error_history)
plt.xlabel('Iteration')
plt.ylabel('Error')
plt.title('Error History')

plt.subplot(2, 1, 2)
plt.plot(acc_history)
plt.xlabel('Iteration')
plt.ylabel('Accuracy')
plt.title('Accuracy History')

plt.tight_layout()
plt.show()

"""4. Plot the data and the trained model. Use the *plot_model* function you implemented before.

**Solution**

Using the *plot_model* function created earlier:
"""

plot_model(best_model,y,labels)

"""5. Retrain the model with a variable learning rate. Use the Scheduler you implemented in Question 1. Plot the dataset and the trained model as before. What do you notice now? Is the difference between training with a fixed and a variable learning rate the same as before? Justify your answer.

**Solution**

Like in Question 1 and using the same Scheduler class, the modified *train_batch* function is:
"""

def train_batch(a, y, labels, n_iterations, lr, variable_lr, batch_size, theta):
    acc_history = []
    error_history = []
    best_accuracy = -1
    best_model = float("-inf")
    
    labels = np.where(labels == 0, -1, labels)
    n_samples = len(labels)
    
    if variable_lr:
      scheduler = Scheduler(lr, n_samples)
    
    for iteration in tqdm(range(n_iterations),desc="Batch perceptron training"):
        error = np.zeros_like(a)
        
        # Randomly select a batch of samples
        indices = np.random.choice(n_samples, size=batch_size, replace=False)
        batch_y = y[indices]
        batch_labels = labels[indices]
        
        for k in range(batch_size):
            x = batch_y[k].astype(a.dtype)
            label = batch_labels[k]
            
            if np.sign(np.dot(a, x)) != label:
                error += x.astype(a.dtype)
        
        if variable_lr:
          lr = scheduler.get_next_lr()

        a += lr * error
        
        if (iteration % (n_samples / batch_size)) == 0:
            accuracy = np.mean([np.sign(np.dot(a, x)) == label for x, label in zip(y, labels)])
            acc_history.append(accuracy)
            tqdm.write(f"Accuracy: {accuracy:.2f}")
            error_history.append(np.abs(np.sum(error)))
            tqdm.write(f"Absolute Error: {np.sum(np.abs(error)):.2f}")


            if accuracy > best_accuracy:
                best_accuracy = accuracy
                best_model = np.copy(a)
        
        if np.sum(np.abs(error)) <= theta:
            tqdm.write("Terminating due to the theta criterion")
            break

        if best_model is None:
         best_accuracy = 0
         best_model = np.zeros_like(a)
    
    return best_model, acc_history, error_history

"""We now train the model again:"""

n_iterations = 10**5
lr = 1
theta = 0.01
batch_size = 16
variable_lr = True

# Train the model using the batch perceptron algorithm
best_model, acc_history, error_history = train_batch(a, y, labels, n_iterations, lr, variable_lr, batch_size, theta)

print(best_model)

plt.subplot(2, 1, 1)
plt.plot(error_history)
plt.xlabel('Iteration')
plt.ylabel('Error')
plt.title('Error History')

plt.subplot(2, 1, 2)
plt.plot(acc_history)
plt.xlabel('Iteration')
plt.ylabel('Accuracy')
plt.title('Accuracy History')

plt.tight_layout()

plt.show()

plot_model(best_model,y,labels)

"""From our data there is no clear difference between training the batch perceptron with a fixed learning rate or a variable learning rate. This may be due to one or more of the following:

- *Choice of the decaying learning rate*. Different learning rates suit different characteristics of the data.

- *Choice of the initial learning rate*. A learning rate that is too large will cause the learning model to compute drastic updates and diverge rapidly, while a too small learning rate will require many updates before reaching a local extrema.

- *Batch size*. Since the selection of the batch influences the noise introduced during the update of the weights, a small batch size leads to more stochastic updates while a larger batch size (compared to the size of the data) will lead to better generalization and smooth out the loss landscape to reduce the presence of local extrema. This may be beneficial, but if the batch size it too large, it can lead to difficulties in finding sharper and more accurate minima that generalize the model in a more optimal way.

Overall, a number of possible parameter configurations (such as the learning rate, fixed or variable learning rate, batch size, theta, epochs (a single pass through all the samples) etc) must be tested and fine-tuned in order to yield the best possible model, often using random walks on the space of possible configurations, called *hyperparameter tuning* [3]. It should be noted again, as stated in the beginning of the assignment, that the given data is not linearly separable, which should also be taken into consideration.



---

### **References**

1. Duda, R. O., & Hart, P. E. (2006). *Pattern classification*. John Wiley & Sons.

2. Wasserman, L. (2004). *All of Statistics: A concise course in Statistical Inference* (Vol. 26). New York: Springer.

3. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
"""